
<h2> Worky Stuff </h2>
<h3>Evidence of a high capacity for innovative thought and problem solving </h3>

<b> Innovative research </b>
For my PhD, I developed an explicit iterative approach to model development in the Land Processes and eXchanges (LPX: <cite>Prentice2011; Kelley2014a</cite>) dynamic global vegetation model (DGVM), in which benchmarking against observations is used to identify areas for new data-driven parameterizations and then subsequently used to evaluate whether the implementation of these new parameterizations produces an overall improvement in model performance. This approach contrasts with the general tendency within the vegetation modelling community to focus evaluation on new components, for example, the evaluation of fire treatments within DGVMs using only observations of burnt area and/or fire carbon fluxes (see e.g. <cite>Li2012; Pfeiffer2013 </cite>). One reason for this partial approach to evaluation was the lack of a comprehensive benchmarking system <cite>Luo2012</cite>). I therefore developed a comprehensive benchmarking system which allows quantitative evaluation of multiple aspects of simulated processes to identify specific model weaknesses and differences between model versions, allowing assessment of the overall impact of new parameterizations. This allowed me to target model development in my thesis . This system is now starting to be adopted by other modelling groups and is being used by a new Model Intercomparison Project to assess the impact of different fire modelling approaches <citep>Hantson</citep>, and to help guide the developmental direction of the fire-vegetation modelling community <citep>Hantson2016</citep>.

<b> Workflow </b>
The fundamentals of any computer programming project is the automation of tasks (imagine trying to solve the state and differential equations in any climate or Earth system model billions of times by hand!). However, this is a principle often forgotten in the vegetation modelling community. This was the case when I inherited the LPX-DGVM. Each new experiment had to be set-up individually - changing initial state conditions, input file names and paths by hand - and then having to manipulate outputs for analysis after the model had completed each run. Applications of LPX always involve multiple runs (e.g, spin-ups and transient runs for multiple climate model inputs in <cite>Ciais2011; Harrisonc<cite>, multiple models, scenarios and CO2 concentrations in <cite>Kelley2014b; Harrison2010</cite>, or different LPX-setups in <cite> Kelley2014a </cite>). I solved this problem by introducing an R and Unix shell-based interface which allows 'switching' between different inputs and/or model configurations, and processing and basic analysis of model outputs based on a small number of interface inputs. This not only saves a huge amount of time, but drastically decreases the possibility of typos and mistakes occuring in runs, and has helped make the model much easier for others to set up and use.

I now try to apply this principle of automation to all of my work. Any code I have developed that I am likely to use again I convert into easily imported libraries or software packages, which can also be easily shared with other people. For examples, <citet>Kelley2015</citet> has been picked up by a model intercomparison project (fireMIP, <cite>Hantson2016</cite>).

<h2> References </h2>
<References> MetOfficeClimMod/refList.bib </References>-->
